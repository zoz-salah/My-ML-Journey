# Neural Network from Scratch with NumPy ğŸ§ 

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.4+-orange.svg)

<img width="791" height="388" alt="image" src="https://github.com/user-attachments/assets/a795ca64-700f-47dd-9d5b-cd7ade396508" />


A complete from-scratch implementation of a Neural Network for classification problems, built layer by layer using only NumPy.

---

## ğŸ“‹ Overview

This implementation demonstrates how to build a fully-connected (dense) neural network from the ground up. You'll understand exactly what happens inside each layer during forward propagation and backpropagation.

---

## ğŸ—ï¸ Neural Network Architecture

### Network Structure

```
INPUT LAYER        HIDDEN LAYERS          OUTPUT LAYER
    [xâ‚] â”€â”€â”€â”€â”€â”€â”   â”Œâ”€[hâ‚]â”€â”   â”Œâ”€[hâ‚]â”€â”   â”Œâ”€[yâ‚]
    [xâ‚‚] â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–ºâ”‚  hâ‚‚  â”‚â”€â”€â–ºâ”‚  hâ‚‚  â”‚â”€â”€â–ºâ”‚  yâ‚‚  â”‚
    [xâ‚ƒ] â”€â”€â”€â”€â”€â”€â”˜   â””â”€[hâ‚ƒ]â”€â”˜   â””â”€[hâ‚ƒ]â”€â”˜   â””â”€[yâ‚ƒ]
    [xâ‚„] â”€â”€â”€â”€â”€â”€â”     Layer 1     Layer 2    Output
    [xâ‚…] â”€â”€â”€â”€â”€â”€â”˜                              (Softmax)
    
Features: n_in   Hidden 1: n_h1   Hidden 2: n_h2   Classes: n_out
```

### Layer-by-Layer Breakdown

#### 1. **Input Layer** (Features)
- **Role**: Receives raw data
- **Size**: Number of features (e.g., 784 for MNIST, 4 for Iris dataset)
- **No computation** - just passes data forward
- **Shape**: (batch_size, n_features)

#### 2. **Hidden Layers**
- **Role**: Learn complex patterns and representations
- **Components**:
  - Linear transformation: `Z = WÂ·X + b`
  - Activation function: `A = activation(Z)`
- **Multiple hidden layers** allow learning hierarchical features

#### 3. **Output Layer** (Predictions)
- **Role**: Produce final classification
- **Size**: Number of classes (e.g., 10 for MNIST digits)
- **Activation**: Softmax (for multi-class) or Sigmoid (for binary)

---


## ğŸ”„ Activation Functions Explained

| Function | Formula | Output Range | Best For | Layer Type |
|----------|---------|--------------|----------|------------|
| **ReLU** | `max(0, x)` | [0, âˆ) | Hidden layers | Prevents vanishing gradient |
| **Sigmoid** | `1/(1+eâ»Ë£)` | (0, 1) | Binary output | Probability output |
| **Tanh** | `(eË£-eâ»Ë£)/(eË£+eâ»Ë£)` | (-1, 1) | Hidden layers | Zero-centered |
| **Softmax** | `eË£/Î£eË£` | (0, 1) | Multi-class output | Probability distribution |

---

