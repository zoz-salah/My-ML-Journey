# Linear Regression with Gradient Descent ğŸ“ˆ
<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/5873f190-7f5c-4454-981f-0733654426ec" />
<img width="2000" height="999" alt="image" src="https://github.com/user-attachments/assets/a7a7d978-8604-4a47-9d5b-5dddd9716e6b" />

A from-scratch implementation of Linear Regression using Gradient Descent optimization algorithm.

---

## ğŸ“‹ Overview

This files implements **Linear Regression** from scratch using only NumPy. It demonstrates:
- The mathematics behind linear regression
- How Gradient Descent works to find optimal parameters
---

## ğŸ§  Theory

### Linear Regression
Linear Regression models the relationship between variables by fitting a linear equation:
```
y = mx + b
```
Where:
- **y**: target/dependent variable
- **x**: feature/independent variable  
- **m**: slope (weight)
- **b**: intercept (bias)

For multiple features:
```
y = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
```

### Gradient Descent
Gradient Descent is an optimization algorithm that iteratively finds the optimal parameters by minimizing the cost function:

1. **Cost Function** (Mean Squared Error):
   ```
   MSE = (1/n) * Î£(y_true - y_pred)Â²
   ```

2. **Gradient Calculation**:
   ```
   âˆ‚MSE/âˆ‚w = (-2/n) * Î£x(y_true - y_pred)
   âˆ‚MSE/âˆ‚b = (-2/n) * Î£(y_true - y_pred)
   ```

3. **Parameter Update**:
   ```
   w = w - Î± * (âˆ‚MSE/âˆ‚w)
   b = b - Î± * (âˆ‚MSE/âˆ‚b)
   ```
   Where **Î±** is the learning rate

---

## ğŸ“ Files

| File | Description |
|------|-------------|
| `linear_regression-with-gradient_descent.py` | Main implementation with visualization |
| `Linear Regression with Gradient Descent.md` | This documentation |

---




